{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import yaml \n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_obj(file_path: str, extension: str, dtype = torch.float32) -> torch.Tensor:\n",
    "    \"\"\"Load a numpy object into a torch tensor\n",
    "    Args:\n",
    "        file_path: str, path to the file\n",
    "        extension: str, file extension to load. Eithere  of .npy or .npz\n",
    "        dtype: torch.dtype, data type to load the data\n",
    "    Returns:\n",
    "        matrix: torch.Tensor, tensor with the data loaded\"\"\"\n",
    "    if extension == \".npy\":\n",
    "        matrix = torch.tensor(np.load(file_path), dtype = dtype)\n",
    "    elif extension == \".npz\":\n",
    "        matrix = torch.tensor(load_npz(file_path).toarray(), dtype = dtype)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, adj_matrix_dir: str, features_dir: str, transform = None):\n",
    "        \"\"\" Initialize the dataset\n",
    "        Args:\n",
    "            adj_matrix_dir: str, path to the directory containing the adjacency matrices\n",
    "            features_dir: str, path to the directory containing the features\n",
    "            transform: callable, transformation to apply to the data\n",
    "        Returns:\n",
    "            (adj_matrix, feature_matrix, label)\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.adj_matrix_dir = adj_matrix_dir\n",
    "        self.features_dir = features_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        label_mapping = {\n",
    "            \"A\": 0,\n",
    "            \"B\": 1,\n",
    "            \"C\": 2,\n",
    "            \"D\": 3,\n",
    "            \"E\": 4,\n",
    "            \"F\": 5,\n",
    "            \"G\": 6,\n",
    "            \"H\": 7,\n",
    "            \"I\": 8,\n",
    "            \"J\": 9, \n",
    "            \"K\": 10,\n",
    "            \"L\": 11,\n",
    "            \"M\": 12,\n",
    "            \"N\": 13,\n",
    "            \"O\": 14,\n",
    "            \"P\": 15,\n",
    "            \"Q\": 16,\n",
    "            \"R\": 17,\n",
    "            \"S\": 18,\n",
    "            \"T\": 19,\n",
    "            \"U\": 20,\n",
    "            \"V\": 21,\n",
    "            \"W\": 22,\n",
    "            \"X\": 23,\n",
    "            \"Y\": 24,\n",
    "            \"Z\": 25,\n",
    "            \"del\": 26,\n",
    "            \"nothing\": 27,\n",
    "            \"space\": 28\n",
    "        }\n",
    "\n",
    "        # Collect all paths and labels withouth loading the data\n",
    "        self.data = []\n",
    "        for label in sorted(os.listdir(self.features_dir)):\n",
    "            adj_label_dir = os.path.join(self.adj_matrix_dir, label)\n",
    "            features_label_dir = os.path.join(self.features_dir, label)\n",
    "            if os.path.isdir(adj_label_dir):\n",
    "                for adj_matrix_file, feature_file in zip(sorted(os.listdir(adj_label_dir)), sorted(os.listdir(features_label_dir))):\n",
    "                    adj_path = os.path.join(adj_label_dir, adj_matrix_file)\n",
    "                    feature_path = os.path.join(features_label_dir, feature_file)\n",
    "                    self.data.append((adj_path, feature_path, label_mapping[label]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the paths to the adj matrix and the features and the corresponding label\n",
    "        adj_path, feature_path, label = self.data[idx]\n",
    "\n",
    "        # Load the data\n",
    "        adj_matrix = load_numpy_obj(adj_path, \".npz\")\n",
    "        feature_matrix = load_numpy_obj(feature_path, \".npy\")\n",
    "\n",
    "        # Apply the transformation\n",
    "        if self.transform:\n",
    "            adj_matrix, feature_matrix = self.transform(adj_matrix, feature_matrix)\n",
    "\n",
    "        return adj_matrix, feature_matrix, label\n",
    "    \n",
    "\n",
    "def load_dataloader(adj_matrix_dir: str, features_dir: str, config: dict):\n",
    "    \"\"\" Load the data as dataloaders and split it into train, val and test sets\n",
    "    Args:\n",
    "        adj_matrix_dir: str, path to the directory containing the adjacency matrices\n",
    "        features_dir: str, path to the directory containing the features\n",
    "        config: dict, configuration dictionary\n",
    "    Returns:\n",
    "        train_data: DataLoader, data loader for the training data\n",
    "        val_data: DataLoader, data loader for the validation data\n",
    "        test_data: DataLoader, data loader for the test data\n",
    "    Dataset format: (adj_matrix, feature_matrix, label)\"\"\"\n",
    "    \n",
    "    dataset = GraphDataset(adj_matrix_dir, features_dir)\n",
    "    train_split, val_split = config.get(\"dataset\").get(\"split_ratio\").get(\"train\"), \\\n",
    "                             config.get(\"dataset\").get(\"split_ratio\").get(\"val\")\n",
    "    \n",
    "    train_data, val_data, test_data = random_split(dataset = dataset,  lengths = [int(train_split*len(dataset)), int(val_split*len(dataset)), \n",
    "                                                                       len(dataset) - int(train_split*len(dataset)) - int(val_split*len(dataset))])\n",
    "    \n",
    "    info = f\"\"\"\n",
    "{\"-*\"*10} Data Information {\"-*\"*10} \\n\n",
    "Total number of samples: {len(dataset)}\n",
    "Number of training samples: {len(train_data)}\n",
    "Number of validation samples: {len(val_data)}\n",
    "Number of test samples: {len(test_data)}\n",
    "{\"-\"*50}\n",
    "\"\"\"\n",
    "    print(info)\n",
    "\n",
    "    batch_Size = config.get(\"training\").get(\"batch_size\")\n",
    "    num_workers = config.get(\"training\").get(\"num_workers\", 0)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size = batch_Size, shuffle = True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_data, batch_size = batch_Size, shuffle = False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_data, batch_size = batch_Size, shuffle = False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.252479</td>\n",
       "      <td>-0.165296</td>\n",
       "      <td>-0.073149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408549</td>\n",
       "      <td>-0.464397</td>\n",
       "      <td>-0.085439</td>\n",
       "      <td>2.756327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432858</td>\n",
       "      <td>-0.715742</td>\n",
       "      <td>-0.102616</td>\n",
       "      <td>2.881634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.401987</td>\n",
       "      <td>-0.909585</td>\n",
       "      <td>-0.105177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.202360</td>\n",
       "      <td>-0.684384</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.291251</td>\n",
       "      <td>-0.831255</td>\n",
       "      <td>-0.142947</td>\n",
       "      <td>1.186180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.304694</td>\n",
       "      <td>-0.612575</td>\n",
       "      <td>-0.224034</td>\n",
       "      <td>2.889807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.303040</td>\n",
       "      <td>-0.422437</td>\n",
       "      <td>-0.245428</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.029805</td>\n",
       "      <td>-0.660269</td>\n",
       "      <td>-0.005085</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.141575</td>\n",
       "      <td>-0.777709</td>\n",
       "      <td>-0.162666</td>\n",
       "      <td>1.202096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.166901</td>\n",
       "      <td>-0.517728</td>\n",
       "      <td>-0.201875</td>\n",
       "      <td>2.875521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.176010</td>\n",
       "      <td>-0.320569</td>\n",
       "      <td>-0.179639</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.132213</td>\n",
       "      <td>-0.613890</td>\n",
       "      <td>-0.032014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001966</td>\n",
       "      <td>-0.646593</td>\n",
       "      <td>-0.186857</td>\n",
       "      <td>1.409971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.027717</td>\n",
       "      <td>-0.397801</td>\n",
       "      <td>-0.163177</td>\n",
       "      <td>2.815724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.027278</td>\n",
       "      <td>-0.236677</td>\n",
       "      <td>-0.094322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.293141</td>\n",
       "      <td>-0.548352</td>\n",
       "      <td>-0.064964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.146309</td>\n",
       "      <td>-0.578647</td>\n",
       "      <td>-0.157048</td>\n",
       "      <td>1.446422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.116231</td>\n",
       "      <td>-0.387528</td>\n",
       "      <td>-0.125498</td>\n",
       "      <td>2.814987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.116881</td>\n",
       "      <td>-0.268380</td>\n",
       "      <td>-0.067939</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x         y         z     angle\n",
       "0   0.000000  0.000000  0.000000  0.000000\n",
       "1   0.252479 -0.165296 -0.073149  0.000000\n",
       "2   0.408549 -0.464397 -0.085439  2.756327\n",
       "3   0.432858 -0.715742 -0.102616  2.881634\n",
       "4   0.401987 -0.909585 -0.105177  0.000000\n",
       "5   0.202360 -0.684384 -0.001825  0.000000\n",
       "6   0.291251 -0.831255 -0.142947  1.186180\n",
       "7   0.304694 -0.612575 -0.224034  2.889807\n",
       "8   0.303040 -0.422437 -0.245428  0.000000\n",
       "9   0.029805 -0.660269 -0.005085  0.000000\n",
       "10  0.141575 -0.777709 -0.162666  1.202096\n",
       "11  0.166901 -0.517728 -0.201875  2.875521\n",
       "12  0.176010 -0.320569 -0.179639  0.000000\n",
       "13 -0.132213 -0.613890 -0.032014  0.000000\n",
       "14  0.001966 -0.646593 -0.186857  1.409971\n",
       "15  0.027717 -0.397801 -0.163177  2.815724\n",
       "16  0.027278 -0.236677 -0.094322  0.000000\n",
       "17 -0.293141 -0.548352 -0.064964  0.000000\n",
       "18 -0.146309 -0.578647 -0.157048  1.446422\n",
       "19 -0.116231 -0.387528 -0.125498  2.814987\n",
       "20 -0.116881 -0.268380 -0.067939  0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = load_numpy_obj(\"/Volumes/Extreme SSD/American_Sign_Language/data/processed_data/extended_features/A/A1_extended_features.npy\", \".npy\")\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(f.numpy())\n",
    "df.columns = [\"x\", \"y\", \"z\", \"angle\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self,input_dim: int,  output_dim: int, *args, **kwargs):\n",
    "        \"\"\" Initialize the GCN layer\n",
    "        Args:\n",
    "            input_dim: int, input dimension of the layer\n",
    "            output_dim: int, output dimension of the layer\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2, \n",
    "                                       inplace=True)\n",
    "        \n",
    "        # Initialize a weight matrix as a parameter\n",
    "        self.W = nn.Parameter(torch.empty(input_dim, output_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "        self.residual_transform = nn.Linear(input_dim, output_dim) if input_dim != output_dim else None\n",
    "\n",
    "    def calculate_degree_matrix(self, A: torch.Tensor):\n",
    "        \"\"\" Calculate the degree matrix of the adjacency matrix\n",
    "        Args:\n",
    "            A: torch.Tensor, adjacency matrix\n",
    "        Returns:\n",
    "            D: torch.Tensor, degree matrix\"\"\"\n",
    "        A_hat = A + torch.eye(A.size(1), device=A.device) # Add self connections\n",
    "\n",
    "        # Degree matrix D and D^{-1/2}\n",
    "        degrees = A_hat.sum(dim=1)\n",
    "        D_neg_sqrt = torch.diag_embed(degrees.pow(-0.5))\n",
    "        return A_hat, D_neg_sqrt\n",
    "    \n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        \"\"\" Forward pass of the GCN layer\n",
    "        Args:\n",
    "            X (torch.Tensor): Input feature matrix of shape (N, F)\n",
    "            A (torch.Tensor): Adjacency matrix of shape (N, N)\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature matrix of shape (N, output_dim)\"\"\"\n",
    "        \n",
    "        A_hat, D_neg_sqrt = self.calculate_degree_matrix(A)  # Compute degree matrix\n",
    "\n",
    "        # Graph convolution operation\n",
    "        support = torch.matmul(D_neg_sqrt, torch.matmul(A_hat, D_neg_sqrt))\n",
    "        output = torch.matmul(support, torch.matmul(X, self.W))\n",
    "\n",
    "        # Residual connection\n",
    "        if self.residual_transform is not None:\n",
    "            X_transformed = self.residual_transform(X)\n",
    "        else:\n",
    "            X_transformed = X\n",
    "        \n",
    "        # Add the residual connection to the output\n",
    "        output += X_transformed\n",
    "\n",
    "        # Activation function\n",
    "        return self.activation(output)\n",
    "    \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, num_classes: int, *args, **kwargs) -> None:\n",
    "        \"\"\" Initialize the GCN model\n",
    "        Args:\n",
    "            input_dim: int, input dimension of the model\n",
    "            hidden_dims: list, list of hidden dimensions\n",
    "            num_classes: int, number of classes in the dataset\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_dim: int = input_dim\n",
    "        self.hidden_dims: int = hidden_dims\n",
    "        self.num_classes: int = num_classes\n",
    "\n",
    "        # Extra layers\n",
    "        self.dropout = nn.Dropout(kwargs.get(\"dropout\", 0.5))\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dims[1])\n",
    "\n",
    "        # Initialize the GCN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(GCNLayer(input_dim, hidden_dim))\n",
    "            self.layers.append(self.dropout)\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # put one batchnorm after 2nd layer\n",
    "        self.layers.insert(3, self.batch_norm) # insert batchnorm after 2nd layer\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward pass of the GCN model\n",
    "        Args:\n",
    "            X (torch.Tensor): Input feature matrix of shape (N, F)\n",
    "            A (torch.Tensor): Adjacency matrix of shape (N, N)\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature matrix of shape (N, num_classes)\"\"\"\n",
    "        for layer in self.layers:\n",
    "\n",
    "            # if layer is dropout or batchnorm, don't pass A\n",
    "            if isinstance(layer, nn.Dropout): X = layer(X)\n",
    "            elif isinstance(layer, nn.BatchNorm1d): \n",
    "                original_shape = X.shape\n",
    "                X = layer(X.view(-1, X.size(-1))) # BatchNorm1d expects (batch_size x num_features)\n",
    "                X = X.view(original_shape) # Reshape back to original shape\n",
    "            else: X = layer(X, A)\n",
    "\n",
    "        # Flatten the output and pass it through the output layer\n",
    "        X = X.view(X.size(0), -1) # first dimension is batch size\n",
    "        output = nn.Linear(X.size(1), self.num_classes)(X)  # Output layer, outputs (batch_size x num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimiser: torch.optim.Optimizer,\n",
    "          criterion: torch.nn.modules.loss._Loss, \n",
    "          train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, \n",
    "          device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "          config: dict = None, **kwargs) -> Tuple[nn.Module, dict]:\n",
    "    \"\"\" Train a model based on the provided configuration in the config file\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model to train\n",
    "        optimiser (torch.optim.Optimizer): Optimiser to use for training\n",
    "        criterion (torch.nn.modules.loss._Loss): Loss function to use for training\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training data\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data\n",
    "        config (dict): Configuration dictionary (default: None)\"\"\"\n",
    "    \n",
    "    # Set up the hyperparameters\n",
    "    if config:\n",
    "        NUM_EPOCHS = config.get(\"training\").get(\"epochs\", 100)\n",
    "        EARLY_STOPPING_PATIENCE = config.get(\"training\").get(\"early_stopping\").get(\"patience\", 15)\n",
    "        MODEL_SAVE_DIR = config.get(\"logging\").get(\"checkpoint_path\", \"Models\") \n",
    "        LOG_DIR = config.get(\"logging\").get(\"log_dir\", \"logs\")\n",
    "    else:\n",
    "        NUM_EPOCHS = kwargs.get(\"epochs\", 100)  \n",
    "        EARLY_STOPPING_PATIENCE = kwargs.get(\"early_stopping_patience\", 15)\n",
    "        MODEL_SAVE_DIR = kwargs.get(\"model_save_dir\", \"Models\")\n",
    "        LOG_DIR = kwargs.get(\"log_dir\", \"logs\")\n",
    "\n",
    "    WEIGHT_DECAY = optimiser.param_groups[0].get(\"weight_decay\", 0)\n",
    "    LEARNING_RATE = optimiser.param_groups[0].get(\"lr\", 0.01)\n",
    "    N_PARAMS = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    \n",
    "    if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)\n",
    "    if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\n",
    "\n",
    "    start_time = time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "    session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_save_path = os.path.join(MODEL_SAVE_DIR, f\"{session_id}.pth\")\n",
    "    logs_save_path = os.path.join(LOG_DIR, f\"{session_id}.pkl\")\n",
    "\n",
    "\n",
    "    training_info = {\n",
    "        \"📅 Training Start Time\": start_time,\n",
    "        \"📈 Total Number of Epochs\": NUM_EPOCHS,\n",
    "        \"💻 Device Used for Training\": device,\n",
    "        \"🆔 Session ID\": session_id,\n",
    "\n",
    "        \"🔢 Number of Trainable Parameters\": N_PARAMS,\n",
    "        \"🚦 Early Stopping Patience\": EARLY_STOPPING_PATIENCE,\n",
    "        \"📉 Weight Decay\": WEIGHT_DECAY,\n",
    "        \"📈 Initial Learning Rate\": LEARNING_RATE,\n",
    "\n",
    "        \"📂 Model Save Path\": model_save_path,\n",
    "        \"📄 Logs Save Path\": logs_save_path\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\n════════════════════════════════════════════\")\n",
    "    print(\"TRAINING SESSION START\")\n",
    "    print(\"════════════════════════════════════════════\")\n",
    "    pprint(training_info)\n",
    "    print(\"════════════════════════════════════════════\")\n",
    "\n",
    "\n",
    "    train_losses, val_losses = [], [] # Lists to store the training and validation losses\n",
    "    best_val_loss = float(\"inf\") # Variable to store the best validation loss\n",
    "    patience = 0 # Variable to store the patience\n",
    "    \n",
    "    model.to(device) # Move the model to the device\n",
    "    loop = tqdm(range(NUM_EPOCHS), desc=\"Training\", position=0, leave=True)\n",
    "    for epoch in loop:\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for adj_matrix, features, labels in train_loader:\n",
    "            features, adj_matrix, labels = features.to(device), adj_matrix.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimiser.zero_grad() # Zero out the gradients\n",
    "            output = model(features, adj_matrix) # Forward pass\n",
    "            loss = criterion(output, labels) # Calculate the loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimiser.step() # Update the weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader) # Calculate the average loss for the epoch\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for adj_matrix, features, labels in val_loader:\n",
    "                features, adj_matrix, labels = features.to(device), adj_matrix.to(device), labels.to(device)\n",
    "                output = model(features, adj_matrix)\n",
    "                loss = criterion(output, labels)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "        avg_val_epoch_loss = val_epoch_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_epoch_loss)\n",
    "\n",
    "        loop.set_description(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_epoch_loss:.4f}, Val Loss: {avg_val_epoch_loss:.4f}, Patience: {patience}\")\n",
    "\n",
    "        if avg_val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_epoch_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    end_time = time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "    training_info = {\n",
    "        \"Training Session Summary\": {\n",
    "            \"📅 Training Start Time\": start_time,\n",
    "            \"⏰ Estimated Training End\": end_time,\n",
    "            \"🆔 Session ID\": session_id\n",
    "        },\n",
    "        \"Training Configuration\": {\n",
    "            \"📈 Number of Epochs Trained\": NUM_EPOCHS,\n",
    "            \"🔢 Number of Trainable Parameters\": N_PARAMS,\n",
    "            \"💻 Device Used for Training\": device,\n",
    "            \"🚦 Early Stopping Patience\": EARLY_STOPPING_PATIENCE,\n",
    "            \"📉 Weight Decay\": WEIGHT_DECAY,\n",
    "            \"📈 Learning Rate\": LEARNING_RATE\n",
    "        },\n",
    "        \"File Paths\": {\n",
    "            \"📂 Model Saved at\": model_save_path,\n",
    "            \"📄 Logs Saved at\": logs_save_path\n",
    "        },\n",
    "        \"Best Performance Metrics\": {\n",
    "            \"🔍 Best Validation Loss\": round(best_val_loss, 4),\n",
    "            \"🔍 Best Training Loss\": round(min(train_losses), 4),\n",
    "            \"🏆 Best Epoch\": best_epoch\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    print(\"\\n\\n════════════════════════════════════════════\")\n",
    "    print(\"TRAINING SESSION END\")\n",
    "    print(\"════════════════════════════════════════════\")\n",
    "    pprint(training_info, width=150)\n",
    "    print(\"════════════════════════════════════════════\")\n",
    "\n",
    "\n",
    "    results = {\n",
    "        \"description\": training_info,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_loss\": best_val_loss, \n",
    "        \"model_path\": model_save_path, \n",
    "    }\n",
    "\n",
    "    # Save the results dictionary in logs \n",
    "    with open(logs_save_path, \"wb\") as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
