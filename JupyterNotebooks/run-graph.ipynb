{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:37.751764Z",
     "iopub.status.busy": "2024-11-30T17:53:37.751178Z",
     "iopub.status.idle": "2024-11-30T17:53:41.336734Z",
     "shell.execute_reply": "2024-11-30T17:53:41.336047Z",
     "shell.execute_reply.started": "2024-11-30T17:53:37.751728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import yaml \n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import time\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:42.403089Z",
     "iopub.status.busy": "2024-11-30T17:53:42.402780Z",
     "iopub.status.idle": "2024-11-30T17:53:42.412191Z",
     "shell.execute_reply": "2024-11-30T17:53:42.411319Z",
     "shell.execute_reply.started": "2024-11-30T17:53:42.403062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'pygments'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"architecture\": \"GraphConvModel\",\n",
    "        \"input_dim\": 4,\n",
    "        \"hidden_dims\": [128, 128, 128, 128, 128],\n",
    "        \"output_dim\": 32,\n",
    "        \"num_classes\": 28,\n",
    "        \"num_layers\": 3,\n",
    "        \"dropout\": 0.5,\n",
    "        \"activation\": \"LeakyReLU\",\n",
    "        \"normalization\": \"batch\"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 10000,\n",
    "        \"batch_size\": 256,\n",
    "        \"num_workers\": 4,\n",
    "        \"learning_rate\": 0.004,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"params\": {\n",
    "                \"betas\": [0.9, 0.999],\n",
    "                \"momentum\": 0.9\n",
    "            }\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"StepLR\",\n",
    "            \"params\": {\n",
    "                \"step_size\": 10,\n",
    "                \"gamma\": 0.5\n",
    "            }\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"patience\": 30\n",
    "        }\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"adj_dir\": \"data/processed_data/adj_matrix\",\n",
    "        \"features_dir\": \"data/processed_data/adj_matrix\",\n",
    "        \"split_ratio\": {\n",
    "            \"train\": 0.85,\n",
    "            \"val\": 0.15,\n",
    "            \"test\": 0.0\n",
    "        },\n",
    "        \"transform\": {\n",
    "            \"normalize\": True,\n",
    "            \"augment\": {\n",
    "                \"rotate\": 15,\n",
    "                \"flip\": True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"log_dir\": \"logs/\",\n",
    "        \"checkpoint_path\": \"Models/\",\n",
    "        \"save_interval\": 5,\n",
    "        \"tensorboard\": True,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"metrics\": [\"accuracy\", \"f1_score\", \"precision\", \"recall\"],\n",
    "        \"confusion_matrix\": True,\n",
    "        \"class_labels\": [\n",
    "            \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \n",
    "            \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"\n",
    "        ]\n",
    "    },\n",
    "    \"seed\": 42,\n",
    "    \"debug_mode\": False,\n",
    "    \"distributed_training\": {\n",
    "        \"enabled\": False,\n",
    "        \"num_nodes\": 1,\n",
    "        \"gpus_per_node\": 1\n",
    "    },\n",
    "    \"custom_params\": {\n",
    "        \"attention_heads\": 4,\n",
    "        \"kernel_size\": 3,\n",
    "        \"pooling_type\": \"max\"\n",
    "    },\n",
    "    \"resume_training\": {\n",
    "        \"checkpoint_file\": \"\",\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:42.415468Z",
     "iopub.status.busy": "2024-11-30T17:53:42.414700Z",
     "iopub.status.idle": "2024-11-30T17:53:42.429563Z",
     "shell.execute_reply": "2024-11-30T17:53:42.428949Z",
     "shell.execute_reply.started": "2024-11-30T17:53:42.415440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_numpy_obj(file_path: str, extension: str, dtype = torch.float32) -> torch.Tensor:\n",
    "    \"\"\"Load a numpy object into a torch tensor\n",
    "    Args:\n",
    "        file_path: str, path to the file\n",
    "        extension: str, file extension to load. Eithere  of .npy or .npz\n",
    "        dtype: torch.dtype, data type to load the data\n",
    "    Returns:\n",
    "        matrix: torch.Tensor, tensor with the data loaded\"\"\"\n",
    "    if extension == \".npy\":\n",
    "        matrix = torch.tensor(np.load(file_path), dtype = dtype)\n",
    "    elif extension == \".npz\":\n",
    "        matrix = torch.tensor(load_npz(file_path).toarray(), dtype = dtype)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "# Collect all paths and labels withouth loading the data\n",
    "def load_data(features_dir, adj_matrix_dir):\n",
    "    label_mapping = {\n",
    "            \"A\": 0,\n",
    "            \"B\": 1,\n",
    "            \"C\": 2,\n",
    "            \"D\": 3,\n",
    "            \"E\": 4,\n",
    "            \"F\": 5,\n",
    "            \"G\": 6,\n",
    "            \"H\": 7,\n",
    "            \"I\": 8,\n",
    "            \"J\": 9, \n",
    "            \"K\": 10,\n",
    "            \"L\": 11,\n",
    "            \"M\": 12,\n",
    "            \"N\": 13,\n",
    "            \"O\": 14,\n",
    "            \"P\": 15,\n",
    "            \"Q\": 16,\n",
    "            \"R\": 17,\n",
    "            \"S\": 18,\n",
    "            \"T\": 19,\n",
    "            \"U\": 20,\n",
    "            \"V\": 21,\n",
    "            \"W\": 22,\n",
    "            \"X\": 23,\n",
    "            \"Y\": 24,\n",
    "            \"Z\": 25,\n",
    "            \"del\": 26,\n",
    "            \"nothing\": 27,\n",
    "            \"space\": 28\n",
    "        }\n",
    "    adj_matrices, features, labels = [], [], []\n",
    "    loop = tqdm(sorted(os.listdir(features_dir)))\n",
    "    for label in loop:\n",
    "            adj_label_dir = os.path.join(adj_matrix_dir, label)\n",
    "            features_label_dir = os.path.join(features_dir, label)\n",
    "            if os.path.isdir(adj_label_dir):\n",
    "                for adj_matrix_file, feature_file in zip(sorted(os.listdir(adj_label_dir)), sorted(os.listdir(features_label_dir))):\n",
    "                    adj_path = os.path.join(adj_label_dir, adj_matrix_file)\n",
    "                    feature_path = os.path.join(features_label_dir, feature_file)\n",
    "                    adj_matrices.append(load_numpy_obj(adj_path, extension = \".npz\"))\n",
    "                    features.append(load_numpy_obj(feature_path, extension = \".npy\"))\n",
    "                    labels.append(label_mapping[label])\n",
    "    return adj_matrices, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:42.430962Z",
     "iopub.status.busy": "2024-11-30T17:53:42.430629Z",
     "iopub.status.idle": "2024-11-30T17:53:42.444024Z",
     "shell.execute_reply": "2024-11-30T17:53:42.443300Z",
     "shell.execute_reply.started": "2024-11-30T17:53:42.430926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, adj_matrices, features, labels, transform = None):\n",
    "        \"\"\" Initialize the dataset\n",
    "        Args:\n",
    "            adj_matrix_dir: str, path to the directory containing the adjacency matrices\n",
    "            features_dir: str, path to the directory containing the features\n",
    "            transform: callable, transformation to apply to the data\n",
    "        Returns:\n",
    "            (adj_matrix, feature_matrix, label)\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.adj_matrices = adj_matrices\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adj_matrices)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Load the adjacency matrix and feature matrix as float tensors\n",
    "        adj_matrix, feature_matrix, label = self.adj_matrices[idx], self.features[idx], self.labels[idx]\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            adj_matrix, feature_matrix = self.transform(adj_matrix, feature_matrix)\n",
    "    \n",
    "        return adj_matrix, feature_matrix, label\n",
    "    \n",
    "\n",
    "def load_dataloader(adj_matrices: list, features: list, labels: list, config: dict):\n",
    "    \"\"\" Load the data as dataloaders and split it into train, val and test sets\n",
    "    Args:\n",
    "        adj_matrix_dir: str, path to the directory containing the adjacency matrices\n",
    "        features_dir: str, path to the directory containing the features\n",
    "        config: dict, configuration dictionary\n",
    "    Returns:\n",
    "        train_data: DataLoader, data loader for the training data\n",
    "        val_data: DataLoader, data loader for the validation data\n",
    "        test_data: DataLoader, data loader for the test data\n",
    "    Dataset format: (adj_matrix, feature_matrix, label)\"\"\"\n",
    "    \n",
    "    dataset = GraphDataset(adj_matrices, features, labels)\n",
    "    train_split, val_split = config.get(\"dataset\").get(\"split_ratio\").get(\"train\"), \\\n",
    "                             config.get(\"dataset\").get(\"split_ratio\").get(\"val\")\n",
    "    \n",
    "    train_data, val_data, test_data = random_split(dataset = dataset,  lengths = [int(train_split*len(dataset)), int(val_split*len(dataset)), \n",
    "                                                                       len(dataset) - int(train_split*len(dataset)) - int(val_split*len(dataset))])\n",
    "    \n",
    "    info = f\"\"\"\n",
    "{\"-*\"*10} Data Information {\"-*\"*10} \\n\n",
    "Total number of samples: {len(dataset)}\n",
    "Number of training samples: {len(train_data)}\n",
    "Number of validation samples: {len(val_data)}\n",
    "Number of test samples: {len(test_data)}\n",
    "{\"-\"*50}\n",
    "\"\"\"\n",
    "    print(info)\n",
    "\n",
    "    batch_Size = config.get(\"training\").get(\"batch_size\")\n",
    "    num_workers = config.get(\"training\").get(\"num_workers\", 0)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_Size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size = batch_Size, shuffle = False)\n",
    "    test_loader = DataLoader(test_data, batch_size = batch_Size, shuffle = False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T18:06:23.365388Z",
     "iopub.status.busy": "2024-11-30T18:06:23.365039Z",
     "iopub.status.idle": "2024-11-30T18:06:23.379120Z",
     "shell.execute_reply": "2024-11-30T18:06:23.378204Z",
     "shell.execute_reply.started": "2024-11-30T18:06:23.365359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        \"\"\"Initialize the GCN layer.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input dimension of the layer.\n",
    "            output_dim (int): Output dimension of the layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # Initialize weight matrix W with Xavier initialization\n",
    "        self.W = nn.Parameter(torch.empty(input_dim, output_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "        # Residual connection transformation (if input and output dimensions differ)\n",
    "        self.residual_transform = nn.Linear(input_dim, output_dim) if input_dim != output_dim else None\n",
    "\n",
    "        # Apply Xavier initialization to residual transform weights\n",
    "        if self.residual_transform is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.residual_transform.weight)\n",
    "            if self.residual_transform.bias is not None:\n",
    "                torch.nn.init.zeros_(self.residual_transform.bias)\n",
    "\n",
    "    def calculate_degree_matrix(self, A: torch.Tensor):\n",
    "        \"\"\"Calculate the normalized adjacency matrix and D^{-1/2}.\n",
    "\n",
    "        Args:\n",
    "            A (torch.Tensor): Adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Normalized adjacency matrix and D^{-1/2}.\n",
    "        \"\"\"\n",
    "        A_hat = A + torch.eye(A.size(1), device=A.device)  # Add self-connections\n",
    "        degrees = A_hat.sum(dim=1)\n",
    "        D_neg_sqrt = torch.diag_embed(degrees.pow(-0.5))\n",
    "        return A_hat, D_neg_sqrt\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        \"\"\"Forward pass of the GCN layer.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input feature matrix of shape (N, F).\n",
    "            A (torch.Tensor): Adjacency matrix of shape (N, N).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature matrix of shape (N, output_dim).\n",
    "        \"\"\"\n",
    "        A_hat, D_neg_sqrt = self.calculate_degree_matrix(A)\n",
    "\n",
    "        # Graph convolution operation\n",
    "        support = torch.matmul(D_neg_sqrt, torch.matmul(A_hat, D_neg_sqrt))\n",
    "        output = torch.matmul(support, torch.matmul(X, self.W))\n",
    "\n",
    "        # Residual connection\n",
    "        if self.residual_transform is not None:\n",
    "            X_transformed = self.residual_transform(X)\n",
    "        else:\n",
    "            X_transformed = X\n",
    "\n",
    "        # Add the residual connection to the output\n",
    "        output += X_transformed\n",
    "\n",
    "        # Activation function\n",
    "        return self.activation(output)\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, num_landmarks: int, num_classes: int, dropout: float = 0.5):\n",
    "        \"\"\"Initialize the GCN model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Input dimension of the model.\n",
    "            hidden_dims (list): List of hidden dimensions.\n",
    "            num_landmarks (int): Number of landmarks per graph.\n",
    "            num_classes (int): Number of classes in the dataset.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Extra layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dims[1])\n",
    "\n",
    "        # Initialize the GCN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_input_dim = input_dim\n",
    "        for idx, hidden_dim in enumerate(hidden_dims):\n",
    "            self.layers.append(GCNLayer(current_input_dim, hidden_dim))\n",
    "            self.layers.append(self.dropout)\n",
    "            current_input_dim = hidden_dim\n",
    "\n",
    "        # Insert batch normalization after the second layer\n",
    "        self.layers.insert(3, self.batch_norm)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(current_input_dim * num_landmarks, num_classes)\n",
    "\n",
    "        # Apply Xavier initialization to the output layer\n",
    "        torch.nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            torch.nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the GCN model.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input feature matrix of shape (batch_size, N, F).\n",
    "            A (torch.Tensor): Adjacency matrix of shape (N, N).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output feature matrix of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # If layer is dropout or batch normalization, don't pass adjacency matrix\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                X = layer(X)\n",
    "            elif isinstance(layer, nn.BatchNorm1d):\n",
    "                original_shape = X.shape\n",
    "                X = layer(X.view(-1, X.size(-1)))  # BatchNorm1d expects (batch_size * N, num_features)\n",
    "                X = X.view(original_shape)         # Reshape back to original shape\n",
    "            else:\n",
    "                X = layer(X, A)\n",
    "\n",
    "        # Flatten the output and pass it through the output layer\n",
    "        X = X.view(X.size(0), -1)  # Flatten features\n",
    "        output = self.output_layer(X)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:42.462213Z",
     "iopub.status.busy": "2024-11-30T17:53:42.461779Z",
     "iopub.status.idle": "2024-11-30T17:53:42.510712Z",
     "shell.execute_reply": "2024-11-30T17:53:42.509763Z",
     "shell.execute_reply.started": "2024-11-30T17:53:42.462177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics_multiclass(y_pred, y_true, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1 score for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): Raw logits or probabilities from the model.\n",
    "        y_true (torch.Tensor): Ground truth class labels (integers 0 to num_classes - 1).\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics - accuracy, precision, recall, F1 score.\n",
    "    \"\"\"\n",
    "    # Convert logits to predicted classes\n",
    "    y_pred_classes = torch.argmax(y_pred, dim=1)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision_per_class = []\n",
    "    recall_per_class = []\n",
    "    f1_per_class = []\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (y_pred_classes == y_true).float().mean().item()\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # True Positives, False Positives, False Negatives\n",
    "        TP = ((y_pred_classes == class_idx) & (y_true == class_idx)).sum().item()\n",
    "        FP = ((y_pred_classes == class_idx) & (y_true != class_idx)).sum().item()\n",
    "        FN = ((y_pred_classes != class_idx) & (y_true == class_idx)).sum().item()\n",
    "\n",
    "        # Precision, Recall, F1 Score for the class\n",
    "        precision = TP / (TP + FP + 1e-7) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN + 1e-7) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-7) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precision_per_class.append(precision)\n",
    "        recall_per_class.append(recall)\n",
    "        f1_per_class.append(f1)\n",
    "    \n",
    "    # Macro-Averaged Metrics\n",
    "    macro_precision = sum(precision_per_class) / num_classes\n",
    "    macro_recall = sum(recall_per_class) / num_classes\n",
    "    macro_f1 = sum(f1_per_class) / num_classes\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": macro_precision,\n",
    "        \"recall\": macro_recall,\n",
    "        \"f1_score\": macro_f1\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, optimiser: torch.optim.Optimizer,\n",
    "          criterion: torch.nn.modules.loss._Loss, \n",
    "          train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, \n",
    "          device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "          config: dict = None, **kwargs) -> Tuple[nn.Module, dict]:\n",
    "    \"\"\" Train a model based on the provided configuration in the config file\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model to train\n",
    "        optimiser (torch.optim.Optimizer): Optimiser to use for training\n",
    "        criterion (torch.nn.modules.loss._Loss): Loss function to use for training\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training data\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data\n",
    "        config (dict): Configuration dictionary (default: None)\"\"\"\n",
    "    \n",
    "    # Set up the hyperparameters\n",
    "    if config:\n",
    "        NUM_EPOCHS = config.get(\"training\").get(\"epochs\", 100)\n",
    "        EARLY_STOPPING_PATIENCE = config.get(\"training\").get(\"early_stopping\").get(\"patience\", 15)\n",
    "        MODEL_SAVE_DIR = config.get(\"logging\").get(\"checkpoint_path\", \"Models\") \n",
    "        LOG_DIR = config.get(\"logging\").get(\"log_dir\", \"logs\")\n",
    "    else:\n",
    "        NUM_EPOCHS = kwargs.get(\"epochs\", 100)  \n",
    "        EARLY_STOPPING_PATIENCE = kwargs.get(\"early_stopping_patience\", 15)\n",
    "        MODEL_SAVE_DIR = kwargs.get(\"model_save_dir\", \"Models\")\n",
    "        LOG_DIR = kwargs.get(\"log_dir\", \"logs\")\n",
    "\n",
    "    WEIGHT_DECAY = optimiser.param_groups[0].get(\"weight_decay\", 0)\n",
    "    LEARNING_RATE = optimiser.param_groups[0].get(\"lr\", 0.01)\n",
    "    N_PARAMS = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    \n",
    "    if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)\n",
    "    if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR)\n",
    "\n",
    "    start_time = time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "    session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_save_path = os.path.join(MODEL_SAVE_DIR, f\"{session_id}.pth\")\n",
    "    logs_save_path = os.path.join(LOG_DIR, f\"{session_id}.pkl\")\n",
    "\n",
    "\n",
    "    training_info = {\n",
    "        \"ğŸ“… Training Start Time\": start_time,\n",
    "        \"ğŸ“ˆ Total Number of Epochs\": NUM_EPOCHS,\n",
    "        \"ğŸ’» Device Used for Training\": device,\n",
    "        \"ğŸ†” Session ID\": session_id,\n",
    "\n",
    "        \"ğŸ”¢ Number of Trainable Parameters\": N_PARAMS,\n",
    "        \"ğŸš¦ Early Stopping Patience\": EARLY_STOPPING_PATIENCE,\n",
    "        \"ğŸ“‰ Weight Decay\": WEIGHT_DECAY,\n",
    "        \"ğŸ“ˆ Initial Learning Rate\": LEARNING_RATE,\n",
    "\n",
    "        \"ğŸ“‚ Model Save Path\": model_save_path,\n",
    "        \"ğŸ“„ Logs Save Path\": logs_save_path\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    print(\"TRAINING SESSION START\")\n",
    "    print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    pprint(training_info)\n",
    "    print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "\n",
    "    train_losses, val_losses = [], [] # Lists to store the training and validation losses\n",
    "    best_val_loss = float(\"inf\") # Variable to store the best validation loss\n",
    "    patience = 0 # Variable to store the patience\n",
    "    \n",
    "    model.to(device) # Move the model to the device\n",
    "    loop = tqdm(range(NUM_EPOCHS), desc=\"Training\", position=0, leave=True)\n",
    "    for epoch in loop:\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        model.to(device)\n",
    "\n",
    "         # Wrap train_loader with tqdm for a single-line progress bar\n",
    "        for adj_matrix, features, labels in train_loader:\n",
    "            # Move tensors to the specified device\n",
    "            features, adj_matrix, labels = features.to(device), adj_matrix.to(device), labels.to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            optimiser.zero_grad()  # Zero out the gradients\n",
    "            output = model(features, adj_matrix)  # Forward pass\n",
    "            loss = criterion(output, labels)  # Calculate the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimiser.step()  # Update the weights\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader) # Calculate the average loss for the epoch\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for adj_matrix, features, labels in val_loader:\n",
    "                features, adj_matrix, labels = features.to(device), adj_matrix.to(device), labels.to(device)\n",
    "                output = model(features, adj_matrix)\n",
    "                loss = criterion(output, labels)\n",
    "                val_epoch_loss += loss.item()\n",
    "\n",
    "        avg_val_epoch_loss = val_epoch_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_epoch_loss)\n",
    "\n",
    "        loop.set_description(f\"Epoch: {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_epoch_loss:.4f}, Val Loss: {avg_val_epoch_loss:.4f}, Patience: {patience}\")\n",
    "\n",
    "        if avg_val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_epoch_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    end_time = time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "    training_info = {\n",
    "        \"Training Session Summary\": {\n",
    "            \"ğŸ“… Training Start Time\": start_time,\n",
    "            \"â° Estimated Training End\": end_time,\n",
    "            \"ğŸ†” Session ID\": session_id, \n",
    "        },\n",
    "        \"File Paths\": {\n",
    "            \"ğŸ“‚ Model Saved at\": model_save_path,\n",
    "            \"ğŸ“„ Logs Saved at\": logs_save_path\n",
    "        },\n",
    "        \"Best Performance Metrics\": {\n",
    "            \"ğŸ” Best Validation Loss\": round(best_val_loss, 4),\n",
    "            \"ğŸ” Best Training Loss\": round(min(train_losses), 4),\n",
    "            \"ğŸ† Best Epoch\": best_epoch\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    print(\"\\n\\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    print(\"TRAINING SESSION END\")\n",
    "    print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    print(training_info)\n",
    "    print(\"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "\n",
    "\n",
    "    results = {\n",
    "        \"description\": training_info,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_loss\": best_val_loss, \n",
    "        \"model_path\": model_save_path, \n",
    "    }\n",
    "\n",
    "    # Save the results dictionary in logs \n",
    "    with open(logs_save_path, \"wb\") as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T17:53:42.512072Z",
     "iopub.status.busy": "2024-11-30T17:53:42.511741Z",
     "iopub.status.idle": "2024-11-30T18:03:35.697848Z",
     "shell.execute_reply": "2024-11-30T18:03:35.696954Z",
     "shell.execute_reply.started": "2024-11-30T17:53:42.512033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed_data/adj_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m BASE_ADJ_MATRIX_PATH \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madj_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/graph-data/processed_data/adj_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m BASE_FEATURES_PATH \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/graph-data/processed_data/extended_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m adj_matrices, features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBASE_FEATURES_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43madj_matrix_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBASE_ADJ_MATRIX_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m load_dataloader(adj_matrices, features, labels, config)\n\u001b[1;32m     10\u001b[0m A, X, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(features_dir, adj_matrix_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m28\u001b[39m\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m adj_matrices, features, labels \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 51\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_dir\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m     53\u001b[0m         adj_label_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(adj_matrix_dir, label)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed_data/adj_matrix'"
     ]
    }
   ],
   "source": [
    "# Setup the data loaders\n",
    "BASE_ADJ_MATRIX_PATH = config.get(\"dataset\").get(\"adj_dir\", \"/kaggle/input/graph-data/processed_data/adj_matrix\")\n",
    "BASE_FEATURES_PATH = config.get(\"dataset\").get(\"features_dir\", \"/kaggle/input/graph-data/processed_data/extended_features\")\n",
    "\n",
    "adj_matrices, features, labels = load_data(features_dir = BASE_FEATURES_PATH, \n",
    "                                          adj_matrix_dir = BASE_ADJ_MATRIX_PATH)\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataloader(adj_matrices, features, labels, config)\n",
    "A, X, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T18:04:02.232016Z",
     "iopub.status.busy": "2024-11-30T18:04:02.231293Z",
     "iopub.status.idle": "2024-11-30T18:04:02.241946Z",
     "shell.execute_reply": "2024-11-30T18:04:02.241117Z",
     "shell.execute_reply.started": "2024-11-30T18:04:02.231985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "INPUT_DIM = config.get(\"model\").get(\"input_dim\", 4)\n",
    "HIDDEN_DIMS = config.get(\"model\").get(\"hidden_dims\", [64, 128, 32])\n",
    "NUM_CLASSES = config.get(\"model\").get(\"num_classes\", 28)\n",
    "NUM_LANDMARKS = A.shape[1]\n",
    "\n",
    "LEARNING_RATE = config.get(\"training\").get(\"learning_rate\", 4e-2)\n",
    "WEIGHT_DECAY = config.get(\"training\").get(\"weight_decay\", 0.001)\n",
    "\n",
    "model = GCN(input_dim=INPUT_DIM, hidden_dims=HIDDEN_DIMS, num_classes=NUM_CLASSES, num_landmarks = 21)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer_name = config.get(\"training\").get(\"optimizer\", \"optimiser\").get(\"type\", \"Adam\")\n",
    "if optimizer_name not in [\"Adam\", \"SGD\"]:\n",
    "    raise ValueError(f\"Optimizer {optimizer_name} not supported. Supported optimizers are Adam and SGD\")\n",
    "\n",
    "if optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "elif optimizer_name == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Setup the loss function\n",
    "loss_fn = nn.NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T18:04:34.319868Z",
     "iopub.status.busy": "2024-11-30T18:04:34.319264Z",
     "iopub.status.idle": "2024-11-30T18:04:34.815209Z",
     "shell.execute_reply": "2024-11-30T18:04:34.813954Z",
     "shell.execute_reply.started": "2024-11-30T18:04:34.319834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Dimensions: [128, 128, 128, 128, 128]\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "TRAINING SESSION START\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "{'ğŸ†” Session ID': '20241130_180434',\n",
      " 'ğŸ’» Device Used for Training': 'cpu',\n",
      " 'ğŸ“‚ Model Save Path': 'Models/20241130_180434.pth',\n",
      " 'ğŸ“„ Logs Save Path': 'logs/20241130_180434.pkl',\n",
      " 'ğŸ“… Training Start Time': '2024-11-30, 18:04:34',\n",
      " 'ğŸ“ˆ Initial Learning Rate': 0.004,\n",
      " 'ğŸ“ˆ Total Number of Epochs': 10000,\n",
      " 'ğŸ“‰ Weight Decay': 0.001,\n",
      " 'ğŸ”¢ Number of Trainable Parameters': 142236,\n",
      " 'ğŸš¦ Early Stopping Patience': 30}\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 28 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHidden Dimensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHIDDEN_DIMS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                           \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 133\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimiser, criterion, train_loader, val_loader, device, config, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero out the gradients\u001b[39;00m\n\u001b[1;32m    132\u001b[0m output \u001b[38;5;241m=\u001b[39m model(features, adj_matrix)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m    134\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    135\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 28 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(f\"Hidden Dimensions: {HIDDEN_DIMS}\")\n",
    "model, results = train_model(model=model, \n",
    "                           optimiser=optimizer, \n",
    "                           criterion=loss_fn, \n",
    "                           train_loader=train_loader, \n",
    "                           val_loader=val_loader, \n",
    "                           config=config, \n",
    "                            device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T18:03:38.301956Z",
     "iopub.status.idle": "2024-11-30T18:03:38.302240Z",
     "shell.execute_reply": "2024-11-30T18:03:38.302117Z",
     "shell.execute_reply.started": "2024-11-30T18:03:38.302102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for adj_matrix, features, labels in train_loader:\n",
    "            # Move tensors to the specified device\n",
    "            features, adj_matrix, labels = features.to(device), adj_matrix.to(device), labels.to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            # optimiser.zero_grad()  # Zero out the gradients\n",
    "            output = model(features, adj_matrix)  # Forward pass\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-30T18:03:38.304026Z",
     "iopub.status.idle": "2024-11-30T18:03:38.304468Z",
     "shell.execute_reply": "2024-11-30T18:03:38.304265Z",
     "shell.execute_reply.started": "2024-11-30T18:03:38.304242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T18:59:51.686461Z",
     "iopub.status.busy": "2024-11-18T18:59:51.686094Z",
     "iopub.status.idle": "2024-11-18T18:59:51.703754Z",
     "shell.execute_reply": "2024-11-18T18:59:51.702597Z",
     "shell.execute_reply.started": "2024-11-18T18:59:51.686430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6030473,
     "sourceId": 9832194,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hamnosym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
